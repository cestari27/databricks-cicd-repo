name: CI Pipeline for Azure Databricks

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install Databricks CLI
        run: pip install databricks-cli

      - name: Configure Databricks CLI
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        run: |
          echo "[DEFAULT]" > ~/.databrickscfg
          echo "host = $DATABRICKS_HOST" >> ~/.databrickscfg
          echo "token = $DATABRICKS_TOKEN" >> ~/.databrickscfg
          chmod 600 ~/.databrickscfg

      - name: Upload notebook to Workspace
        run: |
          databricks workspace mkdirs /Shared/cicd-notebooks || true
          databricks workspace import sample_sales_notebook.py /Shared/cicd-notebooks/sample_sales_notebook --language PYTHON --overwrite

      - name: Verify notebook upload
        run: databricks workspace ls /Shared/cicd-notebooks/

      - name: Create and run job with inline data
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        run: |
          # Modifica o job config para usar dados inline ou de outra fonte
          echo "Job configuration updated to use workspace notebook"
